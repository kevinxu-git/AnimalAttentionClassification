{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>DATA CAMP Team Project : Animal attention classification</h1></center>\n",
    "<br/>\n",
    "<center>Authors : Pierre ADEIKALAM - Guangyue CHEN - Jiabin CHEN - Chuanyuan QIAN - Qin WANG - Kevin XU</center>\n",
    "\n",
    "<img src=\"suicidal-deer-2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M2 Data Science - Universit√© Paris Saclay  \n",
    " \n",
    "**Professors :** Alexandre GRAMFORT (Inria) & Thomas MOREAU (Inria)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Science Case](#BusinessCase)\n",
    "2. [Installation of Librairies](#Imports)\n",
    "3. [Data Exploration](#DataExploration)\n",
    "4. [Baseline Model](#Baselinemodel)\n",
    "5. [Submission on Ramp](#Submission-structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a style=\"color:#920037\"> 1. SCIENCE CASE </a>\n",
    "## <a style=\"color:#920037\"> 1.1 Introduction </a>\n",
    "\n",
    "> One of the greatest challenges of autonomous driving is to teach a system how to deal with uncommon and unexpected events. One kind of such event is the encounter of animals on the road. Due to our poor understanding of animal cognition, it is hard to teach an autonomous system how to deal with this kind of encounter. \n",
    ">\n",
    "> In Sweden, the National Wildlife Accident Council has stated that 58,337 wildlife accidents had been reported to the police in 2016 alone. The large majority of these accidents involve large animals like roe deer (44,456), deer, wild boars and elks (5,846). These animals should have the cognitive abilities to properly assess the threat of incoming vehicles according to (Lima et al, 2015), but the main issue is that **vehicles are not detected before the impact**.\n",
    ">\n",
    "> In some settings, autonomous systems have the ability to avoid crashes with higher precision than humans (Yu and Petnga, 2018), but risky evasive maneuvers need to be deployed as a last resort. In the case of avoiding animal collisions, (Lima et al, 2015) established the following behavior diagram:\n",
    ">\n",
    "><img src=\"behavior.jpg\" width = \"400px\">\n",
    ">\n",
    "> This diagram illustrates the key animal behavioural steps in avoiding a collision with an object on a collision course. If the object is unable to adapt its trajectory, failure from the animal to properly react at any step from detection through evasion will result in a collision. As we cannot expect a fully autonomous car to come to a full stop each time an animal is encountered in the side of the road, **it should be able to evaluate the reaction of the animal at each of these steps** in order to engage in an evasive maneuver if it is needed.\n",
    ">\n",
    "> Therefore, **the first step in order to avoid a collision should be to evaluate if the vehicle has been detected**. As new regulations enforce cars to be as silent as possible and silent electric cars begin to take over, animals have to rely on their **vision** to detect oncoming vehicles. From the perspective of the autonomous system, one effective way to evaluate this is to detect thanks to its cameras if **at some point the animal looked at the car**, which is a typical classification problem. In this challenge, we will try to provide a solution that answers this question.\n",
    ">\n",
    "## <a style=\"color:#920037\"> 1.2 Task and Dataset </a>\n",
    "\n",
    "> This challenge is an image classification problem that should evalute whether or not an animal is looking at the camera. For this, we have created a new dataset from Google Images and labeled each one of them ourselves.\n",
    ">\n",
    "> The training dataset is made of 2945 images of animals that are commonly encountered near roads around the world such as dogs and cats in urban roads, horses, cows, pigs and sheep in rural roads but also more exotic animals such as monkeys, koalas, kanguroos, turtles, llamas found near roads in Australia, Asia and South America.\n",
    ">\n",
    "> The test dataset is made of 280 images and is a lot more diverse, with completely different species. The reason we chose to do this is we would like to know how well this dataset can be generalized to other animals that are not as common near roads but could also be encountered.\n",
    ">\n",
    "> The labels are:\n",
    "> \n",
    "> * 0 : The animal or the majority of the animals is looking away from the camera.\n",
    "> * 1: The animal or the majority of the animals is looking at the camera\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a style=\"color:#920037\"> 2. INSTALLATION OF LIBRAIRIES </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get this starting kit running, the following dependencies are required : \n",
    "\n",
    "- numpy\n",
    "- ipywidgets\n",
    "- matplotlib\n",
    "- pandas\n",
    "- tensorflow\n",
    "- scikit-learn\n",
    "- jupyter\n",
    "- ramp-worflow\n",
    "- os\n",
    "- cv2\n",
    "- googledrivedownload (In order to download the images from Google Drive)\n",
    "\n",
    "We recommend that you use the Python distribution from Anaconda and so to install those using conda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a style=\"color:#920037\"> 3.Data exploration </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a style=\"color:#920037\"> 3.1 First look at the images  </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To download the data, you simply need to run the following script (the googledrivedownload package is needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install googledrivedownloader\n",
    "!python download_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob \n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "path = './data/'\n",
    "image_folder = path + 'images/'\n",
    "train_image_paths = sorted(glob.glob(image_folder + \"*\"))\n",
    "train_labels = pd.read_csv(path + 'train.csv', index_col=False, dtype=object)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the `.csv` file should look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to efficiently look at the images, we have written a widget in the script `exploration_widget.py` that should be present in the same folder as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exploration_widget import create_interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `create_interface()` returns a widget. Its arguments are:\n",
    "* `train_labels`: takes as input a `DataFrame` containing the ids and labels of the images.\n",
    "* `label` : label you would like to look at, i.e 1 if you want to see the images of animals looking at the camera\n",
    "\n",
    "Once you have called the `create_interface()` function and stored its output, you can display the widget by using the `display()` function of the `IPython.display` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interface = create_interface(train_labels, label = 1)\n",
    "display(interface)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The widget has different commands:\n",
    "* The `Prev` button allows you to load the previous picture.\n",
    "* The `Next` button allows you to load the next picture.\n",
    "* The slider allows you to search for a specific image.\n",
    "* The \"Media\" buttons (play, pause, stop, repeat) allow you to see many images in quick succession."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to load the data into an array, we have written the `loading_data()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loading_data(path = './data/', f_name = 'train'):\n",
    "    data_path = os.path.join(path, '{}.csv'.format(f_name))\n",
    "    data = pd.read_csv(data_path)\n",
    "    \n",
    "    X_id = data['id_image'].values\n",
    "    Y = data['label_image'].values\n",
    "\n",
    "    X_data=np.zeros((len(X_id), 360, 640, 3), dtype = int)\n",
    "    \n",
    "    for i,file in enumerate(X_id):\n",
    "        img = cv2.imread(os.path.join(path, \"images\", str(file)))\n",
    "        X_data[i] = img       \n",
    "    \n",
    "    return (np.array(X_data),np.array(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes 2 arguments:\n",
    "* `path`: a string indicating where the data is located.\n",
    "* `f_name`: a string indicating which dataset you would like to load. Either `\"train\"` or `\"test\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = loading_data('./data/', 'train')\n",
    "\n",
    "#X_test, y_test = loading_data('./data/', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train[10])\n",
    "plt.imshow(X_train[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a style=\"color:#920037\"> 4. BASELINE MODEL  </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encourage participants to develop a deep learning approach in order to solve this problem. Due to the low amount of images, we highly recommend to use **transfer learning** in order to produce efficient results. We will show you how easily it can be done with the `keras` deep learning library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import  Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Dropout, BatchNormalization\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a style=\"color:#920037\"> 4.1 Introduction to keras </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us build a very simple Convolutional Neural Network in order to get started with the `keras` package.\n",
    "\n",
    "Building a `Sequential` model means that we will add layers on top of each other as we go. This type of model can be created with the `Sequential()` constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a convolution layer, we use the `Conv2D` constructor. Many layer types are available and all are created using the same style of api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layer1 = Conv2D(filters = 8, kernel_size = 5, activation='relu', input_shape=(360,640,3))\n",
    "conv_layer2 = Conv2D(filters = 16, kernel_size = 3, activation='relu')\n",
    "\n",
    "maxpool_layer = MaxPooling2D(pool_size=2)\n",
    "flatten_layer = Flatten()\n",
    "\n",
    "dense_layer1 = Dense(10,activation = 'relu')\n",
    "dense_layer2 = Dense(1,activation = 'sigmoid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can add these layers to this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(conv_layer1)\n",
    "model.add(conv_layer2)\n",
    "\n",
    "model.add(maxpool_layer)\n",
    "model.add(flatten_layer)\n",
    "\n",
    "model.add(dense_layer1)\n",
    "model.add(dense_layer2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start training, we need to compile this model with an optimizer and a loss function to minimize. Here, we use the `'binary_crossentropy'` loss with the `Adam` optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.Adam(lr=learning_rate),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now fitting the model to the data is done with the `fit` method of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1\n",
    "batch_size = 2\n",
    "\n",
    "model.fit(X_train[:10], y_train[:10],\n",
    "          batch_size=int(batch_size), \n",
    "          epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a style=\"color:#920037\"> 4.1 Transfer learning with keras </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer learning is the technique of using a pretrained deep learning model as a feature extractor and then build a new model on top of it to perform a certain task. The incentive to do this is that features extracted by pretrained models often give a good representation of our images and fitting a deep learning model from this features is considerably easier and requires less data.\n",
    "\n",
    "In this example, we will create a model on top of the VGG16 model trained on the Imagenet dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_tensor=Input(shape=(128,128,3)),classes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to freeze the layers of VGG16 so that we don't alter them while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers:  \n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we use `keras`'s functional API to apply a series of layer operations to an input `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = base_model.output  \n",
    "x = Flatten()(x)  \n",
    "x = Dense(10, activation='elu')(x)\n",
    "x = Dropout(0.4)(x)  \n",
    "x = Dense(10, activation='elu')(x)\n",
    "x = Dropout(0.1)(x)\n",
    "predictions = Dense(1, activation='sigmoid', name='predictions')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the model is just the list of operations that took place between the input of VGG16 and the output of the `predicitons`layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.Adam(lr=learning_rate),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is now ready to be compiled and fitted like the previous one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a style=\"color:#920037\"> 5. SUBMISSION ON RAMP </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a style=\"color:#920037\"> 5.1 Submission files  </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have developed a good solution, you need to implement an `ImageClassifier` class in order to submit it to RAMP.\n",
    "\n",
    "Here, we give an example of an `ImageClassifier` class that implements transfer learning with VGG16.\n",
    "This class has the following methods:\n",
    "\n",
    "* `__init__()`: Constructs the `ImageClassifier` object.\n",
    "* `_transform()`: Transforms an input array to have the right dimensions in order to be passed to the model. Very important when performing transfer learning as the transfered model has a specific input size requirement. \n",
    "* `_build_model()`: Builds the model like we did above.\n",
    "* `fit()` : Fits the model to the data.\n",
    "* `predict_proba()`: Makes a prediction on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import keras\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Dropout, BatchNormalization\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import applications\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class ImageClassifier(object):\n",
    "\n",
    "    def __init__(self, batch_size=64, epochs=2):\n",
    "        \"\"\"The constructor. Defines some training hyperparameters.\"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _transform(self, x):   \n",
    "        \"\"\"Transforms the image so it has a shape of (128, 128, 3) and values between 0 and 1\"\"\"\n",
    "        \"\"\"Any other kind of transformation should be put here.\"\"\"\n",
    "        x = x / 255.\n",
    "        x_resize = cv2.resize(x, (128,128))\n",
    "        return x_resize\n",
    "\n",
    "    def fit(self, img_loader):\n",
    "        \"\"\"Fits the model\"\"\"\n",
    "        nb = len(img_loader)\n",
    "        X = np.zeros((nb, 128, 128, 3))\n",
    "        Y = np.zeros((nb,2))\n",
    "        \n",
    "        for i in range(nb):\n",
    "            x, y = img_loader.load(i)\n",
    "            X[i] = self._transform(x)\n",
    "            Y[i, y] = 1\n",
    "            \n",
    "        self.model.fit(x=X, y=Y, batch_size=self.batch_size, epochs=self.epochs, verbose=1)\n",
    "\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        \"\"\"Performs the prediction.\"\"\"\n",
    "        n = len(X_test)\n",
    "        X = np.zeros((nb, 128, 128, 3))\n",
    "        for i in range(nb):\n",
    "            X[i] = self._transform(X_test[i])\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def _build_model(self):\n",
    "        base_model = applications.VGG16(weights='imagenet', include_top=False, input_tensor=Input(shape=(128,128,3)),classes=2)\n",
    "\n",
    "        for layer in base_model.layers:  \n",
    "            layer.trainable = False\n",
    "        \n",
    "        x = base_model.output  \n",
    "        x = Flatten()(x)  \n",
    "        x = Dense(10, activation='elu')(x)\n",
    "        x = Dropout(0.4)(x)  \n",
    "        x = Dense(10, activation='elu')(x)\n",
    "        x = Dropout(0.1)(x)\n",
    "        predictions = Dense(2, activation = 'softmax', name='predictions')(x)\n",
    "\n",
    "        model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam(lr=1e-3), metrics=['accuracy'])\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only methods of this class that you should change are:\n",
    "\n",
    "* `_build_model()`: To define your model.\n",
    "* `_transform()`: To apply preprocessing transformations to the images.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a style=\"color:#920037\"> 5.2 Local testing before submitted to RAMP  </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have defined your `ImageClassifier`class, you should write it in a python script called `image_classifer.py` and then save this script in a new folder inside the `submissions` folder. For this example, we have named our folder `transfer_learning`.\n",
    "\n",
    "You should test your submission before submitting it to the RAMP website. For this, the `ramp_test_submission.py` is available to you. To use it, make sure that you have installed the `ramp-workflow` package.\n",
    "\n",
    "In order to test your submission, you should run the script like shown below. Once the test has been successfully ran, it should output the cross validation scores of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ramp_test_submission --submission transfer_learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the test worked, you should be ready to submit on the RAMP website. Good luck!"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
